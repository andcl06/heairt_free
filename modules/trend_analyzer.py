# modules/trend_analyzer.py

import re
from collections import Counter
from datetime import datetime, timedelta
import streamlit as st # Streamlitì˜ st.warning ë“±ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ import.
                        # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ì´ ë¡œê¹… ë¶€ë¶„ì„ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
from konlpy.tag import Okt # konlpyì˜ Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì„í¬íŠ¸

# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìˆ˜í–‰)
# Streamlit í™˜ê²½ì—ì„œëŠ” ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ê±°ë‚˜, í•¨ìˆ˜ ë‚´ì—ì„œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”ë˜ë„ë¡ ìºì‹±í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ì§€ë§Œ, ì‹¤ì œ ì•±ì—ì„œëŠ” st.cache_resource ë“±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
try:
    okt = Okt()
    KONLPY_AVAILABLE = True
except Exception as e:
    st.error(f"ğŸš¨ Konlpy (Okt) ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
    st.info("ğŸ’¡ Konlpyë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ Java Development Kit (JDK) 1.8 ì´ìƒì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
    KONLPY_AVAILABLE = False
    okt = None # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •

def extract_keywords_from_text(text: str) -> list[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    konlpy Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³ , ë¶ˆìš©ì–´ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not text:
        return []

    keywords = []
    if KONLPY_AVAILABLE and okt:
        try:
            # Oktë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ë§Œ ì¶”ì¶œ
            nouns = okt.nouns(text)
            
            # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ ëª©ë¡ (í™•ì¥ ê°€ëŠ¥)
            # í˜•íƒœì†Œ ë¶„ì„ í›„ì˜ ëª…ì‚¬ í˜•íƒœë¥¼ ê³ ë ¤í•˜ì—¬ ë¶ˆìš©ì–´ ëª©ë¡ ì¡°ì •
            stopwords = [
                "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì™€", "ê³¼", "ë„", "ë§Œ", "ê³ ", "ì—", "ì˜", "í•œ", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°",
                "ëŒ€í•œ", "í†µí•´", "ì´ë²ˆ", "ì§€ë‚œ", "ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìœ„í•´", "ìœ¼ë¡œ", "ì—ì„œ",
                "ìœ¼ë¡œ", "ë¡œë¶€í„°", "ê¹Œì§€", "ë¶€í„°", "ìœ¼ë¡œ", "í•˜ì—¬", "ì—ê²Œ", "ì²˜ëŸ¼", "ë§Œí¼", "ë“¯ì´", "ë³´ë‹¤", "ì•„ë‹ˆë¼", "ì•„ë‹ˆë©´", "ê·¸ë¦¬ê³ ",
                "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë”°ë¼ì„œ", "ë•Œë¬¸ì—", "ëŒ€í•´", "ê´€ë ¨", "ìµœê·¼", "ì´ë‚ ", "ì˜¤ì „", "ì˜¤í›„", "ê¸°ì", "ë‰´ìŠ¤", "ì—°í•©ë‰´ìŠ¤",
                "ì¡°ì„ ë¹„ì¦ˆ", "í•œê²¨ë ˆ", "ytn", "mbn", "ë‰´ì‹œìŠ¤", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", # ì–¸ë¡ ì‚¬ëª… ì†Œë¬¸ì ì²˜ë¦¬
                "ë…„", "ì›”", "ì¼", "ë•Œ", "ê³³", "ì ", "ë¶„", "ëª…", "ê°œ", "ìœ„", "ë§", "ë’¤", "ì „", "ì¤‘", "ì¸¡", "ë‚´", "ë°–", "ê³ ", "ë°", "ë°”"
            ]

            # ë‘ ê¸€ì ì´ìƒì¸ ëª…ì‚¬ë§Œ í¬í•¨í•˜ê³  ë¶ˆìš©ì–´ ì œê±°
            # ëª…ì‚¬ ì¶”ì¶œ í›„ ì†Œë¬¸ì ë³€í™˜í•˜ì—¬ ë¶ˆìš©ì–´ì™€ ë¹„êµ
            keywords = [
                word.lower() for word in nouns
                if len(word) > 1 and word.lower() not in stopwords
            ]
        except Exception as e:
            st.warning(f"âš ï¸ Konlpy ëª…ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¼ë°˜ í† í°í™”ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ì˜ ê°„ë‹¨í•œ í† í°í™” ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
            text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
            tokens = text.lower().split()
            stopwords = [
                "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì™€", "ê³¼", "ë„", "ë§Œ", "ê³ ", "ì—", "ì˜", "í•œ", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°",
                "ëŒ€í•œ", "í†µí•´", "ì´ë²ˆ", "ì§€ë‚œ", "ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìœ„í•´", "ìœ¼ë¡œ", "ì—ì„œ",
                "ìœ¼ë¡œ", "ë¡œë¶€í„°", "ê¹Œì§€", "ë¶€í„°", "ìœ¼ë¡œ", "í•˜ì—¬", "ì—ê²Œ", "ì²˜ëŸ¼", "ë§Œí¼", "ë“¯ì´", "ë³´ë‹¤", "ì•„ë‹ˆë¼", "ì•„ë‹ˆë©´", "ê·¸ë¦¬ê³ ",
                "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë”°ë¼ì„œ", "ë•Œë¬¸ì—", "ëŒ€í•´", "ê´€ë ¨", "ì§€ë‚œ", "ìµœê·¼", "ì´ë²ˆ", "ì´ë‚ ", "ì˜¤ì „", "ì˜¤í›„", "ì˜¤í›„", "ì˜¤ì „",
                "ê¸°ì", "ë‰´ìŠ¤", "ì—°í•©ë‰´ìŠ¤", "ì¡°ì„ ë¹„ì¦ˆ", "í•œê²¨ë ˆ", "ytn", "mbn", "ë‰´ì‹œìŠ¤", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ"
            ]
            keywords = [word for word in tokens if len(word) > 1 and word not in stopwords]
    else:
        # konlpyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ì˜ ê°„ë‹¨í•œ í† í°í™” ë°©ì‹ ì‚¬ìš©
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
        tokens = text.lower().split()
        stopwords = [
            "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì™€", "ê³¼", "ë„", "ë§Œ", "ê³ ", "ì—", "ì˜", "í•œ", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°",
            "ëŒ€í•œ", "í†µí•´", "ì´ë²ˆ", "ì§€ë‚œ", "ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìœ„í•´", "ìœ¼ë¡œ", "ì—ì„œ",
            "ìœ¼ë¡œ", "ë¡œë¶€í„°", "ê¹Œì§€", "ë¶€í„°", "ìœ¼ë¡œ", "í•˜ì—¬", "ì—ê²Œ", "ì²˜ëŸ¼", "ë§Œí¼", "ë“¯ì´", "ë³´ë‹¤", "ì•„ë‹ˆë¼", "ì•„ë‹ˆë©´", "ê·¸ë¦¬ê³ ",
            "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë”°ë¼ì„œ", "ë•Œë¬¸ì—", "ëŒ€í•´", "ê´€ë ¨", "ì§€ë‚œ", "ìµœê·¼", "ì´ë²ˆ", "ì´ë‚ ", "ì˜¤ì „", "ì˜¤í›„", "ì˜¤í›„", "ì˜¤ì „",
            "ê¸°ì", "ë‰´ìŠ¤", "ì—°í•©ë‰´ìŠ¤", "ì¡°ì„ ë¹„ì¦ˆ", "í•œê²¨ë ˆ", "ytn", "mbn", "ë‰´ì‹œìŠ¤", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ"
        ]
        keywords = [word for word in tokens if len(word) > 1 and word not in stopwords]

    return keywords

def analyze_keyword_trends(articles_metadata: list[dict], recent_days_period: int = 2, total_days_period: int = 15, min_surge_ratio: float = 1.5, min_recent_freq: int = 3) -> list[dict]:
    """
    ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    recent_days_period: íŠ¸ë Œë“œë¥¼ ê°ì§€í•  ìµœê·¼ ê¸°ê°„ (ì˜ˆ: 2ì¼)
    total_days_period: ë¹„êµí•  ì „ì²´ ê¸°ê°„ (ì˜ˆ: 15ì¼)
    min_surge_ratio: ìµœê·¼ ê¸°ê°„ ë¹ˆë„ / ê³¼ê±° ê¸°ê°„ ë¹ˆë„ ë¹„ìœ¨ì´ ì´ ê°’ ì´ìƒì¼ ë•Œ íŠ¸ë Œë“œë¡œ ê°„ì£¼
    min_recent_freq: ìµœê·¼ ê¸°ê°„ì— ìµœì†Œí•œ ì´ íšŸìˆ˜ ì´ìƒ ì–¸ê¸‰ë˜ì–´ì•¼ íŠ¸ë Œë“œë¡œ ê°„ì£¼
    ë°˜í™˜ ê°’: [{keyword: str, recent_freq: int, past_freq: int, surge_ratio: float}]
    """
    if not articles_metadata:
        return []

    today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

    recent_articles = []
    past_articles = []

    for article in articles_metadata:
        article_date = article.get("ë‚ ì§œ")
        if not isinstance(article_date, datetime):
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨í•œ ê²½ìš°, ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°„ì£¼í•˜ì—¬ ì²˜ë¦¬ (ì •í™•ë„ ë‚®ìŒ)
            st.warning(f"ê²½ê³ : '{article['ì œëª©']}' ê¸°ì‚¬ì˜ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨. ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°„ì£¼í•˜ì—¬ ë¶„ì„ì— í¬í•¨í•©ë‹ˆë‹¤.")
            article_date = today

        if today - timedelta(days=recent_days_period) <= article_date:
            recent_articles.append(article)
        elif today - timedelta(days=total_days_period) <= article_date < today - timedelta(days=recent_days_period):
            past_articles.append(article)

    # ê° ê¸°ê°„ì˜ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
    recent_keywords = Counter()
    for article in recent_articles:
        # íŠ¸ë Œë“œ ë¶„ì„ ì‹œ ì œëª©ê³¼ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí« ëª¨ë‘ í™œìš©
        text_for_keywords = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "") # 'ë‚´ìš©'ì´ ì´ì œ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí«
        recent_keywords.update(extract_keywords_from_text(text_for_keywords)) # í•¨ìˆ˜ëª… ë³€ê²½ ì ìš©

    past_keywords = Counter()
    for article in past_articles:
        # íŠ¸ë Œë“œ ë¶„ì„ ì‹œ ì œëª©ê³¼ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí« ëª¨ë‘ í™œìš©
        text_for_keywords = article["ì œëª©"] + " " + article.get("ë‚´ìš©", "") # 'ë‚´ìš©'ì´ ì´ì œ ë¯¸ë¦¬ë³´ê¸° ìŠ¤ë‹ˆí«
        past_keywords.update(extract_keywords_from_text(text_for_keywords)) # í•¨ìˆ˜ëª… ë³€ê²½ ì ìš©

    trending_keywords_list = [] # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€ê²½
    for keyword, recent_freq in recent_keywords.items():
        past_freq = past_keywords.get(keyword, 0) # ê³¼ê±° ê¸°ê°„ì— ì—†ìœ¼ë©´ 0

        # ìµœê·¼ ê¸°ê°„ì— ìµœì†Œ ë¹ˆë„ ì´ìƒì´ì–´ì•¼ í•¨
        if recent_freq < min_recent_freq:
            continue

        surge_ratio = None
        if past_freq == 0:
            # ê³¼ê±°ì— ì—†ì—ˆëŠ”ë° ìµœê·¼ì— ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œëŠ” íŠ¸ë Œë“œë¡œ ê°„ì£¼
            if recent_freq >= min_recent_freq:
                surge_ratio = float('inf') # ë¬´í•œëŒ€ë¡œ í‘œí˜„
        else:
            surge_ratio = recent_freq / past_freq
            if surge_ratio < min_surge_ratio: # ìµœì†Œ ì¦ê°€ìœ¨ ë¯¸ë‹¬ ì‹œ íŠ¸ë Œë“œ ì•„ë‹˜
                continue

        trending_keywords_list.append({
            "keyword": keyword,
            "recent_freq": recent_freq,
            "past_freq": past_freq,
            "surge_ratio": surge_ratio
        })

    # ë¹ˆë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    trending_keywords_list = sorted(trending_keywords_list, key=lambda x: x['recent_freq'], reverse=True)

    return trending_keywords_list
